{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_23328\\817426989.py:15: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.5)\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_23328\\817426989.py:16: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_23328\\817426989.py:47: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  intent_chain = LLMChain(llm=llm, prompt=intent_prompt)\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_23328\\817426989.py:116: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eal_set\\data\u000bector_store\\webtoon_vector_store\n",
      "eal_set\\data\u000bector_store\\webnovel_vector_store\n",
      "eal_set\\data\u000bector_store\\total_vector_store\n",
      "일부 벡터 스토어가 존재하지 않습니다. 먼저 벡터 스토어를 생성하세요.\n",
      "\n",
      "[의도 파악] 사용자 입력: 요즘 10대들은 뭘 많이 봐?\n",
      "[분류된 의도]: 사용자 입력 \"요즘 10대들은 뭘 많이 봐?\"는 심층적 분류가 필요합니다. 따라서 의도 번호는 3-1입니다. 유도 질문으로 \"10대들이 좋아하는 웹툰이나 웹소설을 찾고 계신가요?\"라고 물어볼 수 있습니다.\n",
      "\n",
      "\n",
      "=== 최종 응답 ===\n",
      "\n",
      "적절한 검색 데이터를 찾을 수 없습니다.\n",
      "\n",
      "=================\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# OpenAI LLM 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.5)\n",
    "\n",
    "# 사용자 질문 입력\n",
    "query = input(\"질문을 입력하세요: \").strip()\n",
    "if not query:\n",
    "    print(\"유효한 질문을 입력하세요.\")\n",
    "    exit()\n",
    "\n",
    "# 사용자 입력의 의도를 분류하는 프롬프트\n",
    "query_type_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"\"\"\n",
    "    사용자의 입력을 다음 5가지 중 하나로 분류하세요: \n",
    "    1-1. 웹툰 추천 \n",
    "\n",
    "    1-2. 웹소설 추천\n",
    "\n",
    "    1-3. 웹툰, 웹소설 함께 추천\n",
    "    (예시)\n",
    "    - \"재밌는 무협 추천해줘\" -> \"다음과 같은 작품들을 추천드릴게요! (작품 추천) 웹툰과 웹소설 중 선택해 주시면 더 자세히 추천드릴 수 있습니다.\"\n",
    "    - \"로판 웹툰이나 웹소설 추천해줘\" -> \"다음과 같은 작품들을 추천드릴게요!\"\n",
    "\n",
    "    2-1. 일상 대화 -> 추천과 연관 지을 수 있음\n",
    "    (예시)\n",
    "    - \"아 회사 가기 싫다.\" -> \"출근은 언제나 힘들죠😭 출근길에 볼만한 코미디 일상물 웹툰을 추천드릴게요! (작품 추천)\"\n",
    "    - \"어우 졸려.\" -> \"잠을 확!!! 깨게 만드는 흥미진진한 웹툰을 추천드릴게요. (작품 추천)\"\n",
    "    - \"햄버거 너무 맛있다.\" -> \"맛있는 햄버거를 드셨나보군요! 부럽네요~🍔 먹음직스러운 음식이 나오는 웹툰 어떠세요~? (작품 추천)\"\n",
    "    - \"아 주식 개망했다.\" -> \"쉽지 않죠...ㅎㅎ 평범했던 주인공이 재벌 급으로 부자가 되는 웹툰을 추천드릴게요. 다시 의욕이 생길거에요!!\"\n",
    "\n",
    "    2-2. 일상 대화 -> 추천과 연관 지을 수 없음\n",
    "    - \"너는 진보야 보수야\" -> \"죄송합니다. 저는 정치적 견해를 가지고 있지 않습니다. 다른 질문을 주시면 웹툰, 웹소설을 추천해 드릴게요.\"\n",
    "    - \"20*30-10는 뭐야?\" -> \"590입니다. 웹툰, 웹소설과 관련된 질문을 주시면 추천해 드리겠습니다.\"\n",
    "\n",
    "    사용자 입력: {query}\n",
    "    의도 번호:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "query_type_chain = LLMChain(llm=llm, prompt=query_type_prompt)\n",
    "response = query_type_chain.run(query)\n",
    "\n",
    "# webtoon db 검색 tool\n",
    "@tool\n",
    "def search_webtoon(query: str) -> list[Document]:\n",
    "    \"\"\"\n",
    "    웹툰 검색\n",
    "    \"\"\"\n",
    "    result = retriever.invoke(query)\n",
    "    return result if result else [Document(page_content=\"검색 결과가 없습니다.\")]\n",
    "\n",
    "# webnovel db 검색 tool\n",
    "@tool\n",
    "def search_webnovel(query: str) -> list[Document]:\n",
    "    \"\"\"\n",
    "    웹소설 검색\n",
    "    \"\"\"\n",
    "    result = retriever.invoke(query)\n",
    "    return result if result else [Document(page_content=\"검색 결과가 없습니다.\")]\n",
    "\n",
    "# web 검색 tool\n",
    "@tool\n",
    "def search_web(query: str) -> list[Document]:\n",
    "    \"\"\"\n",
    "    웹 검색\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tavily_search = TavilySearchResults(max_results=2)\n",
    "        result = tavily_search.invoke(query)\n",
    "        if result:\n",
    "            return [\n",
    "                Document(\n",
    "                    page_content=item.get(\"content\", \"\"),\n",
    "                    metadata={\"title\": item.get(\"title\", \"\")},\n",
    "                )\n",
    "                for item in result\n",
    "            ]\n",
    "        else:\n",
    "            return [Document(page_content=\"검색 결과가 없습니다.\")]\n",
    "    except Exception as e:\n",
    "        return [Document(page_content=f\"오류 발생: {str(e)}\")]\n",
    "\n",
    "\n",
    "# 사용자 입력을 처리하는 함수\n",
    "def handle_user_query(user_query):\n",
    "    try:\n",
    "        intent_response = query_type_chain.invoke({\"query\": user_query})\n",
    "        intent_number = intent_response[\"text\"].strip()  \n",
    "    except Exception as e:\n",
    "        return f\"의도 분석 중 오류 발생: {str(e)}\"\n",
    "\n",
    "    print(f\"\\n[의도 파악] 사용자 입력: {user_query}\")\n",
    "    print(f\"[분류된 의도]: {intent_number}\\n\")\n",
    "\n",
    "    # 의도에 따라 적절한 벡터 스토어 선택\n",
    "    if intent_number in [\"1-1\", \"2-1\"]:\n",
    "        retriever = webtoon_vector_store.as_retriever()\n",
    "    elif intent_number == \"1-2\":\n",
    "        retriever = webnovel_vector_store.as_retriever()\n",
    "    elif intent_number == \"1-3\":\n",
    "        retriever = combined_vector_store.as_retriever()\n",
    "    else:\n",
    "        return \"적절한 검색 데이터를 찾을 수 없습니다.\"\n",
    "\n",
    "    # 검색 실행\n",
    "    docs = retriever.get_relevant_documents(user_query)\n",
    "\n",
    "    # 검색 결과가 없을 경우 처리\n",
    "    if not docs:\n",
    "        print(\"\\n[검색 결과 없음]\\n\")\n",
    "        return \"관련된 정보를 찾을 수 없습니다. 다시 질문해 주세요.\"\n",
    "\n",
    "    print(f\"\\n[검색된 문서 개수]: {len(docs)}\")\n",
    "    for idx, doc in enumerate(docs[:3]):  # 상위 3개만 출력\n",
    "        print(f\"\\n[문서 {idx+1}]:\\n{doc.page_content}\\n\")\n",
    "\n",
    "    # 검색 결과를 LLM에 전달하여 최종 응답 생성\n",
    "    context_data = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    if not context_data.strip():\n",
    "        context_data = \"추천 데이터를 찾을 수 없습니다.\"\n",
    "\n",
    "    final_prompt = f\"사용자의 의도: {intent_number}\\n입력된 질문: {user_query}\\n추천 데이터:\\n{context_data}\"\n",
    "    \n",
    "    try:\n",
    "        final_response = llm.invoke(final_prompt) \n",
    "    except Exception as e:\n",
    "        final_response = f\"LLM 응답 생성 중 오류 발생: {str(e)}\"\n",
    "\n",
    "    return final_response\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "parser = StrOutputParser()\n",
    "\n",
    "\n",
    "\n",
    "response = handle_user_query(query)\n",
    "\n",
    "print(\"\\n=== 최종 응답 ===\\n\")\n",
    "print(response)\n",
    "print(\"\\n=================\")\n",
    "\n",
    "print(\"\\n=================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
