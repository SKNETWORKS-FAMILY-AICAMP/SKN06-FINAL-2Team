import os
import json
import re
import requests
import time
from datetime import datetime
from bs4 import BeautifulSoup as bs
import pickle
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.action_chains import ActionChains
from load_detail import thumbnail_crawling
from dotenv import load_dotenv


################################################################################################################
# kakao_login
################################################################################################################

def get_webtoon_urls_by_tab(tab_name, driver):
    """
    íŠ¹ì • ìš”ì¼ ë˜ëŠ” 'ì™„ê²°' íƒ­ì—ì„œ 'ì „ì²´' ë²„íŠ¼ì„ í´ë¦­í•œ í›„, ëª¨ë“  ì›¹íˆ° URLì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    """
    base_url = f"https://webtoon.kakao.com/?tab={tab_name}"
    driver.get(base_url)
    time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

    print(f"{tab_name.upper()} íƒ­ í¬ë¡¤ë§ ì‹œì‘...")

    # "ì „ì²´" ë²„íŠ¼ í´ë¦­ (ë²„íŠ¼ì´ ë¡œë“œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦° í›„ í´ë¦­)
    try:
        entire_button = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.XPATH, "//button[span[text()='ì „ì²´']]"))
        )
        entire_button.click()
        time.sleep(2)  # í´ë¦­ í›„ í˜ì´ì§€ ë³€ê²½ ëŒ€ê¸°
    except Exception as e:
        print(f"'{tab_name.upper()}' íƒ­ì—ì„œ 'ì „ì²´' ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ í´ë¦­í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", e)

    # ë¬´í•œ ìŠ¤í¬ë¡¤ (ìŠ¤í¬ë¡¤ì„ ëê¹Œì§€ ë‚´ë ¤ì„œ ëª¨ë“  ì›¹íˆ° ë¡œë“œ)
    last_height = driver.execute_script("return document.body.scrollHeight")
    
    while True:
        driver.find_element(By.TAG_NAME, "body").send_keys(Keys.END)  # í˜ì´ì§€ ëìœ¼ë¡œ ìŠ¤í¬ë¡¤
        time.sleep(2)  # ë°ì´í„° ë¡œë”© ëŒ€ê¸°
        new_height = driver.execute_script("return document.body.scrollHeight")
        
        if new_height == last_height:  # ë” ì´ìƒ ìŠ¤í¬ë¡¤ì´ ë‚´ë ¤ê°€ì§€ ì•Šìœ¼ë©´ ì¢…ë£Œ
            break
        last_height = new_height

    # ì›¹íˆ° ëª©ë¡ì—ì„œ URL ê°€ì ¸ì˜¤ê¸°
    webtoon_urls = []
    webtoon_elements = driver.find_elements(By.CSS_SELECTOR, "a[href*='/content/']")  # ì›¹íˆ° ë§í¬ ìš”ì†Œ ì°¾ê¸°
    
    for element in webtoon_elements:
        webtoon_url = element.get_attribute("href")  # ì›¹íˆ° URL ê°€ì ¸ì˜¤ê¸°
        if webtoon_url and webtoon_url.startswith("https://webtoon.kakao.com/content/"):
            webtoon_urls.append(webtoon_url)

    print(f"{tab_name.upper()} íƒ­ì—ì„œ {len(webtoon_urls)}ê°œì˜ ì›¹íˆ°ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\n")
    return list(set(webtoon_urls))  # ì¤‘ë³µ ì œê±° í›„ ë°˜í™˜


def get_all_webtoon_urls():
    """
    Seleniumì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ìš”ì¼ ë° ì™„ê²° íƒ­ì—ì„œ ì›¹íˆ° URLì„ ê°€ì ¸ì˜¤ê³  í”¼í´ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    """
    #Chrome WebDriver ì„¤ì •
    options = webdriver.ChromeOptions()
    options.add_argument("--headless") 
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    # ğŸ”¹ ìš”ì¼ + ì™„ê²° íƒ­ ëª©ë¡
    tabs = ["mon", "tue", "wed", "thu", "fri", "sat", "sun", "complete"]
    all_webtoon_urls = []

    for tab in tabs:
        urls = get_webtoon_urls_by_tab(tab, driver)
        all_webtoon_urls.extend(urls)

    driver.quit()  # ë¸Œë¼ìš°ì € ì¢…ë£Œ

    all_webtoon_urls = list(set(all_webtoon_urls))  # ì¤‘ë³µ ì œê±°

    # ğŸ”¹ ì›¹íˆ° URL ë¦¬ìŠ¤íŠ¸ë¥¼ í”¼í´ íŒŒì¼ë¡œ ì €ì¥
    with open("webtoon_kko_urls.pkl", "wb") as f:
        pickle.dump(all_webtoon_urls, f)

    print(f"ì›¹íˆ° URLì´ 'kakao_webtoon_urls.pkl' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    return all_webtoon_urls

# í¬ë¡¤ë§ ì‹¤í–‰ ë° ì €ì¥
webtoon_list = get_all_webtoon_urls()

################################################################################################################
# webtoon_detail_crawling
################################################################################################################

load_dotenv()
# WebDriver ì„¤ì •
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))

kakao_id = os.getenv("KAKAO_ID")
kakao_pw = os.getenv("KAKAO_PWD")
driver.get("https://webtoon.kakao.com/more")

# ë¡œê·¸ì¸ì²˜ë¦¬
login_icon = r"#root > main > div > div.bg-background-02 > div > div > a:nth-child(1) > div > img.w-113.h-38"
driver.find_element(By.CSS_SELECTOR, login_icon).click()
time.sleep(2)
login_button = r"body > div:nth-child(5) > div > div > div > div.overflow-x-hidden.overflow-y-auto.\!overflow-hidden.flex.flex-col > div.text-center.pb-\[112px\].overflow-y-auto > div > div > button"
driver.find_element(By.CSS_SELECTOR, login_button).click()
time.sleep(2)

ID_selector = r"#loginId--1"
PWD_selector = r"#password--2"
ID_input = driver.find_element(By.CSS_SELECTOR, ID_selector)
ActionChains(driver).send_keys_to_element(ID_input, kakao_id).perform()
PWD_input = driver.find_element(By.CSS_SELECTOR, PWD_selector)
ActionChains(driver).send_keys_to_element(PWD_input, kakao_pw).perform()
login_selector = (
    r"#mainContent > div > div > form > div.confirm_btn > button.btn_g.highlight.submit"
)
driver.find_element(By.CSS_SELECTOR, login_selector).click()
time.sleep(2)
# ë™ì‹œì ‘ì† ê¸°ê¸° íšŸìˆ˜ ì´ˆê³¼ ì‹œ

alert = r"body > div:nth-child(8) > div > div > div > div.absolute.w-full.px-18.bottom-0.pb-30.flex.left-0.z-1.bg-grey-01.light\:bg-white.Alert_buttonsWrap__2ln9d.pt-10 > button.relative.px-10.py-0.btn-white.light\:btn-black.button-active"
driver.find_element(By.CSS_SELECTOR, alert).click()


def extract_webtoon_id(url):
    """ì¹´ì¹´ì˜¤ì›¹íˆ° URLì—ì„œ ì›¹íˆ° IDë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    match = re.search(r'/content/.+/(\d+)', url)
    return match.group(1) if match else None

# ìˆ«ì ë³€í™˜
def convert_views(view_text):
    if "ë§Œ" in view_text:
        return int(float(view_text.replace(",", "").replace("ë§Œ", "")) * 10000)
    elif "ì–µ" in view_text:
        return int(float(view_text.replace(",", "").replace("ì–µ", "")) * 100000000)
    else:
        return int(view_text.replace(",", "")) if view_text.replace(",", "").isdigit() else "-"

def get_webtoon_info(webtoon_id):
    """APIë¥¼ ì‚¬ìš©í•´ ì›¹íˆ° ê¸°ë³¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ê³ , Seleniumì„ ì‚¬ìš©í•´ ì¶”ê°€ ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜"""
    profile_url = f"https://gateway-kw.kakao.com/decorator/v2/decorator/contents/{webtoon_id}/profile"
    episode_url = f"https://gateway-kw.kakao.com/episode/v2/views/content-home/contents/{webtoon_id}/episodes?sort=-NO&offset=0&limit=30"
    price_url = f"https://gateway-kw.kakao.com/ticket/v2/views/content-home/available-tickets?contentId={webtoon_id}"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36",
        "Referer": "https://webtoon.kakao.com/",
        "Accept-Language": "ko",
    }

    # Seleniumìœ¼ë¡œ API URL ì§ì ‘ ì—´ê¸°
    time.sleep(2)
    driver.get(profile_url)
    time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

    # HTMLì—ì„œ JSON ì¶”ì¶œ
    soup = bs(driver.page_source, "html.parser")
    time.sleep(2)
    pre_tag = soup.find("pre")  # API ì‘ë‹µì´ JSON í˜•íƒœë¡œ ë‹´ê²¨ ìˆìŒ
    if not pre_tag:
        print(f"API ì‘ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {webtoon_id}")
        return None

    # JSON íŒŒì‹±
    try:
        data = json.loads(pre_tag.text)  # Seleniumì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ë³€í™˜
    except json.JSONDecodeError:
        print(f"JSON ë””ì½”ë”© ì‹¤íŒ¨: {webtoon_id}")
        return None

    # ì›¹íˆ° ì •ë³´ íŒŒì‹±
    seo_id = data.get("data", {}).get("seoId", "")
    webtoon_page_url = f"https://webtoon.kakao.com/content/{seo_id}/{webtoon_id}?tab=profile"

    # ì‘ê°€, ì¼ëŸ¬ìŠ¤íŠ¸ë ˆì´í„°, ì›ì‘ ë¶„ë¦¬
    author, illustrator, original = "-", "-", "-"
    for person in data.get("data", {}).get("authors", []):
        if person.get("type") == "AUTHOR":
            author = person.get("name", "-")
        elif person.get("type") == "ILLUSTRATOR":
            illustrator = person.get("name", "-")
        elif person.get("type") == "ORIGINAL_STORY":
            original = person.get("name", "-")

    # ì—°ì¬ ìƒíƒœ ì„¤ì •
    status = "-"
    status_map = {
        "COMPLETED": "ì™„ê²°",
        "END_OF_SEASON": "ì—°ì¬",
        "SEASON_COMPLETED":"ì—°ì¬",
        "EPISODES_PUBLISHING": "ì—°ì¬",
        "EPISODES_NOT_PUBLISHING": "íœ´ì¬"
    }

    for badge in data.get("data", {}).get("badges", []):
        if badge.get("type") == "STATUS":
            status = status_map.get(badge.get("title"), "-")

    # ì—°ì¬ ìš”ì¼ ë³€í™˜
    weekdays_map = {
        "MON": "ì›”ìš”ì¼", "TUE": "í™”ìš”ì¼", "WED": "ìˆ˜ìš”ì¼",
        "THU": "ëª©ìš”ì¼", "FRI": "ê¸ˆìš”ì¼", "SAT": "í† ìš”ì¼", "SUN": "ì¼ìš”ì¼"
    }
    update_days = "-"
    if status not in ["íœ´ì¬", "ì™„ê²°"]:
        update_days = [
            weekdays_map.get(b.get("title", "").upper(), "-")
            for b in data.get("data", {}).get("badges", []) if b.get("type") == "WEEKDAYS"
        ]
        update_days = ", ".join(update_days) if update_days else "-"

    # ì—°ë ¹ ì œí•œ
    age_rating = "ì „ì²´ ì´ìš©ê°€"
    for badge in data.get("data", {}).get("badges", []):
        if badge.get("type") == "AGE_LIMIT":
            age_rating = f"{badge.get('title', '')}ì„¸ ì´ìš©ê°€"

    # í‚¤ì›Œë“œ ì •ë¦¬
    keywords = ", ".join([k.replace("#", "") for k in data.get("data", {}).get("seoKeywords", [])])
    
    # APIì—ì„œ ì—í”¼ì†Œë“œ ê°œìˆ˜ì™€ ìµœì´ˆ ì—°ì¬ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
    episode_response = requests.get(episode_url, headers=headers)
    
    episode_count = episode_response.json().get("meta", {}).get("pagination", {}).get("totalCount", 0) if episode_response.status_code == 200 else 0
    
    first_episode = "-"
    episode_date = episode_response.json().get("data", {}).get("first", {}).get("serialStartDateTime", None)
    if episode_date:
        first_episode = datetime.strptime(episode_date[:10], "%Y-%m-%d").strftime("%Y.%m.%d") 
    
    # ê°€ê²© ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    response = requests.get(price_url, headers=headers)
    price_info = response.json().get("data", {})
    free_publishing = data.get("freePublishing", None)
    free_episode_count = data.get("freeEpisodeCount", 0) 
    
    price = "-"
    if "waitForFree" in price_info:
        price = "ê¸°ë‹¤ë¦¬ë©´ ë¬´ë£Œ"
    elif free_publishing is False and free_episode_count == episode_count:
        price = "ë¬´ë£Œ"
    elif free_publishing is False:
        price = "ìœ ë£Œ"
    else:
        price = "ë¬´ë£Œ"  # ê¸°ë³¸ì ìœ¼ë¡œ ë¬´ë£Œ ì²˜ë¦¬

    # Seleniumì„ ì´ìš©í•œ ì¶”ê°€ ì •ë³´ í¬ë¡¤ë§
    time.sleep(3)
    driver.get(webtoon_page_url)
    time.sleep(2)

    soup = bs(driver.page_source, "html.parser")

    # ì¥ë¥´, ì¡°íšŒìˆ˜, ì¢‹ì•„ìš” ê°€ì ¸ì˜¤ê¸°
    genre, views, likes = "-", "-", "-"
    stats = soup.find_all("div", class_="flex justify-center items-start h-14 mt-8 leading-14")

    for stat in stats:
        items = stat.find_all("p", class_="whitespace-pre-wrap break-all break-words support-break-word s12-regular-white ml-2 opacity-75")
        if len(items) >= 3:
            genre = items[0].text.strip()  # ì¥ë¥´
            views = convert_views(items[1].text.strip())  # ì¡°íšŒìˆ˜
            likes = convert_views(items[2].text.strip())  # ì¢‹ì•„ìš”

    # ì›¹íˆ° ì •ë³´ ì •ë¦¬
    webtoon_info = {
        "id": int(webtoon_id),
        "type": "ì›¹íˆ°",
        "platform": "ì¹´ì¹´ì˜¤ì›¹íˆ°",
        "title": data.get("data", {}).get("title", ""),
        "status": status,
        "update_days": update_days,
        "thumbnail": "-",
        "genre": genre,
        "views": views,
        "likes": likes,
        "synopsis": data.get("data", {}).get("synopsis", ""),
        "keywords": keywords,
        "author": author,
        "illustrator": illustrator,
        "original": original,
        "age_rating": age_rating,
        "price": price,
        "episode": episode_count,
        "url": webtoon_page_url,
        "first_episode": first_episode
    }
    return webtoon_info

# í”¼í´ íŒŒì¼ì—ì„œ ì›¹íˆ° URL ë¶ˆëŸ¬ì˜¤ê¸°
with open("webtoon_kko_urls.pkl", "rb") as f:
    webtoon_urls = pickle.load(f)

# ì›¹íˆ° ì •ë³´ í¬ë¡¤ë§ ì‹¤í–‰
webtoon_data_list = []
for url in webtoon_urls[:]:  # í• ë§Œí¼ ì§€ì •
    webtoon_id = extract_webtoon_id(url)
    if webtoon_id:
        webtoon_data = get_webtoon_info(webtoon_id)
        if webtoon_data:
            webtoon_data_list.append(webtoon_data)
            print(f"ì´ {len(webtoon_data_list)}ê°œì˜ ì›¹íˆ° ì •ë³´ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


# JSON íŒŒì¼ë¡œ ì €ì¥
with open("detail/webtoon_kko_crawling.json", "w", encoding="utf-8") as f:
    json.dump(webtoon_data_list, f, ensure_ascii=False, indent=4) # <- ì´ ë°ì´í„° jsonìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°

print(f"{webtoon_id}ì˜ ì›¹íˆ° ì •ë³´ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


################################################################################################################
# webtoon_detail_crawling
################################################################################################################

thumbnail_crawling()
print("thumbnail crawling ì¶”ê°€ ì™„ë£Œ")


