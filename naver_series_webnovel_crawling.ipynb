{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CRAWLING   카테고리에서 장르만 / top100 월간/주간?  장르별 링크\n",
    "- 01_로맨스 ~ 1833  top100  주간 > https://series.naver.com/novel/top100List.series?rankingTypeCode=WEEKLY&categoryCode=201\n",
    "\n",
    "- 02_로판 ~ 619  top100  주간 > https://series.naver.com/novel/top100List.series?rankingTypeCode=WEEKLY&categoryCode=207\n",
    "\n",
    "- 03_판타지 ~493   top100  주간 > https://series.naver.com/novel/top100List.series?rankingTypeCode=WEEKLY&categoryCode=202\n",
    "\n",
    "- 04_현판 ~401  top100  주간 > https://series.naver.com/novel/top100List.series?rankingTypeCode=WEEKLY&categoryCode=208\n",
    "\n",
    "- 05_무협 ~218  top100  주간 > https://series.naver.com/novel/top100List.series?rankingTypeCode=WEEKLY&categoryCode=206\n",
    "\n",
    "- n06_미스터리 ~288  top100  월간  8편 > https://series.naver.com/novel/top100List.series?rankingTypeCode=MONTHLY&categoryCode=203\n",
    "- 06_미스터리 ~288  장르 평점순  12편 https://series.naver.com/novel/categoryProductList.series?categoryTypeCode=genre&genreCode=203&orderTypeCode=star_score&is&isFinished=false\n",
    "\n",
    "- 07_라이트노벨 ~39  top100  월간  11편 > https://series.naver.com/novel/top100List.series?rankingTypeCode=MONTHLY&categoryCode=205\n",
    "- 07_라이트노벨 ~39  장르 평점순  9편 > https://series.naver.com/novel/categoryProductList.series?categoryTypeCode=genre&genreCode=205&orderTypeCode=star_score&is&isFinished=false\n",
    "\n",
    "- 08_BL ~401   top100  주간 > https://series.naver.com/novel/top100List.series?rankingTypeCode=WEEKLY&categoryCode=209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ https://series.naver.com/novel/categoryProductList.series?categoryTypeCode=genre&genreCode=203&orderTypeCode=star_score&is&isFinished=false → Top 리스트 크롤링 실패 → 평점순 크롤링 실행\n",
      "⚠️ https://series.naver.com/novel/categoryProductList.series?categoryTypeCode=genre&genreCode=205&orderTypeCode=star_score&is&isFinished=false → Top 리스트 크롤링 실패 → 평점순 크롤링 실행\n",
      "✅ 총 185개의 소설 URL이 저장되었습니다.\n",
      "📁 JSON 파일 경로: sample/wn_ns_sample_20250217_143846.json\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "\n",
    "# 📌 크롤링할 URL 리스트\n",
    "url_list = [\n",
    "    \"https://series.naver.com/novel/top100List.series?rankingTypeCode=WEEKLY&categoryCode=201\",\n",
    "    \"https://series.naver.com/novel/top100List.series?rankingTypeCode=WEEKLY&categoryCode=207\",\n",
    "    \"https://series.naver.com/novel/top100List.series?rankingTypeCode=WEEKLY&categoryCode=202\",\n",
    "    \"https://series.naver.com/novel/top100List.series?rankingTypeCode=WEEKLY&categoryCode=208\",\n",
    "    \"https://series.naver.com/novel/top100List.series?rankingTypeCode=WEEKLY&categoryCode=206\",\n",
    "    \"https://series.naver.com/novel/top100List.series?rankingTypeCode=MONTHLY&categoryCode=203\",\n",
    "    \"https://series.naver.com/novel/categoryProductList.series?categoryTypeCode=genre&genreCode=203&orderTypeCode=star_score&is&isFinished=false\",\n",
    "    \"https://series.naver.com/novel/top100List.series?rankingTypeCode=MONTHLY&categoryCode=205\",\n",
    "    \"https://series.naver.com/novel/categoryProductList.series?categoryTypeCode=genre&genreCode=205&orderTypeCode=star_score&is&isFinished=false\",\n",
    "    \"https://series.naver.com/novel/top100List.series?rankingTypeCode=WEEKLY&categoryCode=209\",\n",
    "]\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# 📌 HTML 가져오기\n",
    "def fetch_html(url):\n",
    "    try:\n",
    "        req = urllib.request.Request(url, headers=headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            return response.read()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ [오류] {url} 크롤링 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "# 📌 소설 URL 크롤링 함수 (Top 리스트 크롤링 실패 시 평점순 크롤링)\n",
    "def get_novel_urls(url):\n",
    "    sourcecode = fetch_html(url)\n",
    "    if sourcecode is None:\n",
    "        return []  # 크롤링 실패 시 빈 리스트 반환\n",
    "\n",
    "    soup = BeautifulSoup(sourcecode, \"html.parser\")\n",
    "    novel_urls = set()  # 중복 방지\n",
    "\n",
    "    # ✅ 1차 크롤링: 주간 Top 리스트\n",
    "    for li in soup.select(\"div.comic_lst_thum_wrap ul.comic_top_lst li\"):\n",
    "        link = li.find(\"a\", class_=\"pic\")\n",
    "        if link and \"href\" in link.attrs:\n",
    "            full_url = urljoin(\"https://series.naver.com\", link[\"href\"])\n",
    "            novel_urls.add(full_url)\n",
    "\n",
    "    # 🔄 만약 주간 Top 리스트 크롤링이 실패했다면? 👉 평점순 크롤링 실행\n",
    "    if not novel_urls:\n",
    "        print(f\"⚠️ {url} → Top 리스트 크롤링 실패 → 평점순 크롤링 실행\")\n",
    "        for li in soup.select(\"div.lst_thum_wrap ul.lst_list li\"):\n",
    "            link = li.find(\"a\", class_=\"pic\")\n",
    "            if link and \"href\" in link.attrs:\n",
    "                href_value = link[\"href\"]\n",
    "                if \"categoryProductList.series\" not in href_value:\n",
    "                    full_url = urljoin(\"https://series.naver.com\", href_value)\n",
    "                    novel_urls.add(full_url)\n",
    "\n",
    "    return list(novel_urls)\n",
    "\n",
    "# 📌 JSON 저장 함수\n",
    "def save_json(path: str, file_name: str, data: list):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(os.path.join(path, file_name), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# ✅ 실행\n",
    "if __name__ == \"__main__\":\n",
    "    all_novel_urls = set()  # 전체 크롤링된 URL 저장 (중복 방지)\n",
    "\n",
    "    for url in url_list:\n",
    "        novel_urls = get_novel_urls(url)\n",
    "        all_novel_urls.update(novel_urls)  # 새로운 URL 추가\n",
    "\n",
    "    # 저장 경로 생성\n",
    "    DATA_DIR = \"sample\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # 파일명 자동 변경\n",
    "    file_name = f\"wn_ns_sample_{timestamp}.json\"\n",
    "\n",
    "    # JSON 파일 저장\n",
    "    save_json(DATA_DIR, file_name, list(all_novel_urls))\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"✅ 총 {len(all_novel_urls)}개의 소설 URL이 저장되었습니다.\")\n",
    "    print(f\"📁 JSON 파일 경로: {DATA_DIR}/{file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 페이지 요소 추출  (미완성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import lxml\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "naver_id = os.getenv(\"NAVER_ID\")\n",
    "naver_pw = os.getenv(\"NAVER_PWD\")\n",
    "\n",
    "def save_cookies(driver, path):\n",
    "    with open(path, \"wb\") as file:\n",
    "        pickle.dump(driver.get_cookies(), file)\n",
    "\n",
    "def load_cookies(driver, path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as file:\n",
    "            cookies = pickle.load(file)\n",
    "            for cookie in cookies:\n",
    "                driver.add_cookie(cookie)\n",
    "\n",
    "def login_naver(driver):\n",
    "    driver.get(\"https://nid.naver.com/nidlogin.login\")\n",
    "    time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "    driver.execute_script(\"document.getElementById('id').value = arguments[0]\", naver_id)\n",
    "    driver.execute_script(\"document.getElementById('pw').value = arguments[0]\", naver_pw)\n",
    "    driver.find_element(By.ID, \"log.login\").click()\n",
    "    time.sleep(5)  # 로그인 완료 대기\n",
    "    save_cookies(driver, \"naver_cookies.pkl\")\n",
    "\n",
    "def initialize_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # 브라우저 창을 띄우지 않음\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "def crawling(url, driver):\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # 페이지 로딩 대기\n",
    "    \n",
    "    series_novel = {\n",
    "        \"type\": \"웹소설\",\n",
    "        \"platform\": \"네이버 시리즈\"\n",
    "    }\n",
    "\n",
    "    \n",
    "    try:\n",
    "        title_selector = r\"#content > div.end_head > h2\"\n",
    "        title_element = driver.find_element(By.CSS_SELECTOR, title_selector).text\n",
    "    except:\n",
    "        title_element = \"-\"\n",
    "    series_novel[\"title\"] = title_element\n",
    "\n",
    "\n",
    "    try:\n",
    "        status_selector = r\"#content > ul.end_info.NE\\=a\\:nvi > li > ul > li:nth-child(1) > span\"\n",
    "        status_element = driver.find_element(By.CSS_SELECTOR, status_selector).text\n",
    "    except:\n",
    "        status_element = \"-\"\n",
    "    series_novel[\"status\"] = status_element\n",
    "    \n",
    "\n",
    "    try:\n",
    "        thumbnail_selector = r\"#container > div.aside.NE\\\\=a\\\\:nvi > span.pic_area > img\"\n",
    "        thumbnail_element = driver.find_element(By.CSS_SELECTOR, thumbnail_selector).get_attribute(\"src\")\n",
    "    except:\n",
    "        try:\n",
    "            thumbnail_selector = r'//*[@id=\"container\"]/div[1]/a/img'\n",
    "            thumbnail_element = driver.find_element(By.XPATH, thumbnail_selector).get_attribute(\"src\")\n",
    "        except:\n",
    "            try:\n",
    "                thumbnail_selector = r'//*[@id=\"container\"]/div[1]/span/img'\n",
    "                thumbnail_element = driver.find_element(By.XPATH, thumbnail_selector).get_attribute(\"src\")\n",
    "            except:\n",
    "                thumbnail_element = \"-\"\n",
    "    series_novel[\"thumbnail\"] = thumbnail_element\n",
    "\n",
    "\n",
    "    try:\n",
    "        genre_selector = r\"#content > ul.end_info.NE\\=a\\:nvi > li > ul > li:nth-child(2) > span > a\"\n",
    "        genre_element = driver.find_element(By.CSS_SELECTOR, genre_selector).text\n",
    "    except:\n",
    "        genre_element = \"-\"\n",
    "    series_novel[\"genre\"] = genre_element\n",
    "\n",
    "\n",
    "    try:\n",
    "        views_selector = r\"-\"\n",
    "        views_element = driver.find_element(By.CSS_SELECTOR, views_selector).text\n",
    "    except:\n",
    "        views_element = \"-\"\n",
    "    series_novel[\"views\"] = views_element\n",
    "\n",
    "\n",
    "    try:\n",
    "        rating_selector = r\"#content > div.end_head > div.score_area > em\"\n",
    "        rating_element = driver.find_element(By.CSS_SELECTOR, rating_selector).text\n",
    "        rating_element = float(rating_element)\n",
    "    except:\n",
    "        rating_element = \"-\"\n",
    "    series_novel[\"rating\"] = rating_element\n",
    "\n",
    "\n",
    "    try:\n",
    "        like_selector = r\"#content > div.end_head > div.user_action_area > ul > li:nth-child(2) > div > a > em\"\n",
    "        like_element = driver.find_element(By.CSS_SELECTOR, like_selector).text\n",
    "        like_element = int(like_element.replace(\",\", \"\"))   \n",
    "    except:\n",
    "        like_element = \"-\"\n",
    "    series_novel[\"like\"] = like_element\n",
    "\n",
    "\n",
    "    try:\n",
    "        CLICKER_SELECTOR = \"#content > div.end_dsc > div:nth-child(1) > span > a\"\n",
    "        driver.find_element(By.CSS_SELECTOR, CLICKER_SELECTOR).click()\n",
    "        synopsis_selector = \"#content > div.end_dsc.open > div:nth-child(2)\"\n",
    "        synopsis_element = driver.find_element(By.CSS_SELECTOR, synopsis_selector).text\n",
    "    except:\n",
    "        try:\n",
    "            synopsis_selector = r'//*[@id=\"content\"]/div[2]/div'\n",
    "            synopsis_element = driver.find_element(By.XPATH, synopsis_selector).text\n",
    "        except:\n",
    "            synopsis_element = \"-\"\n",
    "    series_novel[\"synopsis\"] = synopsis_element\n",
    "\n",
    "    try:\n",
    "        keywords_selector = r\"-\"\n",
    "        keywords_element = driver.find_element(By.CSS_SELECTOR, keywords_selector).text\n",
    "        keywords_element = keywords_element.replace(\"#\", \"\")\n",
    "        keywords_element = keywords_element.split(\"\\n\")\n",
    "    except:\n",
    "        keywords_element = \"-\"\n",
    "    series_novel[\"keywords\"] = keywords_element\n",
    "\n",
    "\n",
    "    try:\n",
    "        author_selector = r\"#content > ul.end_info.NE\\=a\\:nvi > li > ul > li:nth-child(3) > a\"\n",
    "        author_element = driver.find_element(By.CSS_SELECTOR, author_selector).text\n",
    "    except:\n",
    "        author_element = \"-\"\n",
    "    series_novel[\"author\"] = author_element\n",
    "\n",
    "\n",
    "    try:\n",
    "        illustrator_selector = r\"-\"\n",
    "        illustrator_element = driver.find_element(By.CSS_SELECTOR, illustrator_selector).text\n",
    "    except:\n",
    "        illustrator_element = \"-\"\n",
    "    series_novel[\"illustrator\"] = illustrator_element\n",
    "\n",
    "\n",
    "    try:                     \n",
    "        age_rating_selector = r\"#content > ul.end_info.NE\\=a\\:nvi > li > ul > li:nth-child(6)\"\n",
    "        age_rating_element = driver.find_element(By.CSS_SELECTOR, age_rating_selector).text\n",
    "    except:\n",
    "        try:\n",
    "            age_rating_selector = r\"#content > ul.end_info.NE\\=a\\:nvi > li > ul > li:nth-child(6)\"\n",
    "            age_rating_element = driver.find_element(By.CSS_SELECTOR, age_rating_selector).text\n",
    "            exclude_keywords = [\"그림\", \"출판사\"]\n",
    "            if not any(keyword in  age_rating_element for keyword in exclude_keywords):\n",
    "                age_rating_element = int(age_rating_element)\n",
    "        except:\n",
    "            try:\n",
    "                age_rating_selector = r\"#content > ul.end_info.NE\\=a\\:nvi > li > ul > li:nth-child(5)\"\n",
    "                age_rating_element = driver.find_element(By.CSS_SELECTOR, age_rating_selector).text\n",
    "                exclude_keywords = [\"그림\", \"출판사\"]\n",
    "                if not any(keyword in  age_rating_element for keyword in exclude_keywords):\n",
    "                    age_rating_element = int(age_rating_element)\n",
    "                else:\n",
    "                    age_rating_element = \"-\"\n",
    "            except:\n",
    "                try:\n",
    "                    age_rating_selector = r'//*[@id=\"content\"]/ul[1]/li/ul/li[5]' \n",
    "                    age_rating_element = driver.find_element(By.XPATH, age_rating_selector).text\n",
    "                    exclude_keywords = [\"그림\", \"출판사\"]\n",
    "                except:\n",
    "                    age_rating_element = \"-\"\n",
    "    series_novel[\"age_rating\"] = age_rating_element\n",
    "\n",
    "    free_selector = r\"#content > div.end_title_fixed_price_sign.NE\\=a\\:nvi > div > div.area_text_information > strong\"\n",
    "    wait_selector = r\"#container > div.aside.NE\\=a\\:nvi > span > em.ico_end_head.daily_free2\"\n",
    "    price_selector = r\"#content > div.area_ebook_price_information > div > dl > dd > div > div.area_price\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        driver.find_element(By.CSS_SELECTOR, free_selector)\n",
    "        price_element = driver.find_element(By.CSS_SELECTOR, free_selector).text.get_attribute(\"무료\")\n",
    "    except:\n",
    "        try:\n",
    "            driver.find_element(By.CSS_SELECTOR, wait_selector)\n",
    "            price_element = driver.find_element(By.CSS_SELECTOR, wait_selector).text\n",
    "        except:\n",
    "            try:\n",
    "                price_element = driver.find_element(By.CSS_SELECTOR, price_selector).text\n",
    "            except:\n",
    "                price_element = \"-\"\n",
    "    series_novel[\"price\"] = price_element\n",
    "    \n",
    "\n",
    "    try:\n",
    "        episode_selector = r\"#content > h5 > strong\"\n",
    "        episode_element = driver.find_element(By.CSS_SELECTOR, episode_selector).text\n",
    "        episode_element[\"episode\"] = int(episode_element)\n",
    "    except:\n",
    "        try:\n",
    "            episode_selector = \"#content > h5.end_total_episode > strong\"\n",
    "            episode_element = driver.find_element(By.CSS_SELECTOR, episode_selector).text\n",
    "            episode_element = int(episode_element)\n",
    "        except:\n",
    "            episode_element = \"-\"\n",
    "    series_novel[\"episode\"] = episode_element\n",
    "    \n",
    "    URL = url + \"?tab_type=overview\"\n",
    "    driver.get(URL)\n",
    "    series_novel[\"url\"] = url\n",
    "    \n",
    "    return series_novel\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    driver = initialize_driver()\n",
    "    \n",
    "    # 쿠키 로드 시도\n",
    "    driver.get(\"https://www.naver.com\")\n",
    "    load_cookies(driver, \"naver_cookies.pkl\")\n",
    "    driver.get(\"https://nid.naver.com/nidlogin.login\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    if \"nid.naver.com\" in driver.current_url:  # 로그인 유지가 안 된 경우 로그인 실행\n",
    "        login_naver(driver)\n",
    "    \n",
    "    url_list = []\n",
    "    result = crawling(url, driver)\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    # 크롤링 결과 저장\n",
    "    with open(\"crawling_result.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(\"크롤링 결과 저장 완료!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
